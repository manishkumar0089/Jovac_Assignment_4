{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40f22387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQues.1 > What is the core assumption of Naive Bayes?\\nAns.> Naive Bayes assumes that all features are conditionally independent of each other given\\n the class label. This \"naive\" assumption simplifies computation and makes the model scalable, \\n even though it rarely holds true in practice.\\n\\nQues.2 > Differentiate between GaussianNB, MultinomialNB, and BernoulliNB.\\nAns.> GaussianNB assumes features follow a normal (Gaussian) distribution and is used for continuous data.\\n\\nMultinomialNB is designed for discrete features like word counts and is commonly used in text classification.\\n\\nBernoulliNB works with binary/boolean features, focusing on the presence or absence of terms.\\n\\nQues.3 > Why is Naive Bayes considered suitable for high-dimensional data?\\nAns.> Naive Bayes is efficient and performs well with high-dimensional data because it\\nassumes feature independence, reducing the complexity of parameter estimation. Its simplicity makes\\nit less prone to overfitting even with a large number of features.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 1: Theory Questions \n",
    "'''\n",
    "Ques.1 > What is the core assumption of Naive Bayes?\n",
    "Ans.> Naive Bayes assumes that all features are conditionally independent of each other given\n",
    " the class label. This \"naive\" assumption simplifies computation and makes the model scalable, \n",
    " even though it rarely holds true in practice.\n",
    "\n",
    "Ques.2 > Differentiate between GaussianNB, MultinomialNB, and BernoulliNB.\n",
    "Ans.> GaussianNB assumes features follow a normal (Gaussian) distribution and is used for continuous data.\n",
    "\n",
    "MultinomialNB is designed for discrete features like word counts and is commonly used in text classification.\n",
    "\n",
    "BernoulliNB works with binary/boolean features, focusing on the presence or absence of terms.\n",
    "\n",
    "Ques.3 > Why is Naive Bayes considered suitable for high-dimensional data?\n",
    "Ans.> Naive Bayes is efficient and performs well with high-dimensional data because it\n",
    "assumes feature independence, reducing the complexity of parameter estimation. Its simplicity makes\n",
    "it less prone to overfitting even with a large number of features.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df3d5b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 2: Spam Detection using MultinomialNB\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset directly from a public URL\n",
    "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
    "df = pd.read_csv(url, sep='\\t', header=None, names=['label', 'message'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610a4105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert 'ham'/'spam' to binary labels\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.2, random_state=42)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0cda7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-win_amd64.whl (11.2 MB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\manish kumar\\miniconda3\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.13.1-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.6.1 scipy-1.13.1 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89cf2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Train model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vec, y_train)\n",
    "y_pred = model.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96638716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9919\n",
      "Precision: 1.0000\n",
      "Recall: 0.9396\n",
      "Confusion Matrix:\n",
      " [[966   0]\n",
      " [  9 140]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95929f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: GaussianNB with Iris Dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load Iris data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5422fb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_gnb = gnb.predict(X_test)\n",
    "acc_gnb = accuracy_score(y_test, y_pred_gnb)\n",
    "\n",
    "print(f\"GaussianNB Accuracy: {acc_gnb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9077ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 1.0000\n",
      "Decision Tree Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=200)\n",
    "lr.fit(X_train, y_train)\n",
    "acc_lr = accuracy_score(y_test, lr.predict(X_test))\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "acc_dt = accuracy_score(y_test, dt.predict(X_test))\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {acc_lr:.4f}\")\n",
    "print(f\"Decision Tree Accuracy: {acc_dt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6e22e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison on Iris Dataset:\n",
      "GaussianNB         : 1.0000\n",
      "Logistic Regression: 1.0000\n",
      "Decision Tree      : 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Comparison on Iris Dataset:\")\n",
    "print(f\"GaussianNB         : {acc_gnb:.4f}\")\n",
    "print(f\"Logistic Regression: {acc_lr:.4f}\")\n",
    "print(f\"Decision Tree      : {acc_dt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35843c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
